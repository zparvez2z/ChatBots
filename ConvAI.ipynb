{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvAI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zparvez2z/ChatBots/blob/master/ConvAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQptjl8jsRH1",
        "colab_type": "text"
      },
      "source": [
        "# Hugging Face QA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5E-v4HxKuu_",
        "colab_type": "code",
        "outputId": "11fc78fb-b27b-4e06-f7be-970ced3db457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xgVgSh3LThh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM82IFywVyrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csen_SoDLUI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel,BertForQuestionAnswering"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIir4Bf1LXtj",
        "colab_type": "code",
        "outputId": "1bf59aa6-8d87-4886-91a2-62e7ad5b52d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULbtHErbLcbJ",
        "colab_type": "code",
        "outputId": "7a5d06bc-badf-4fa8-fff0-378aeae24100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "# question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
        "input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\n",
        "input_ids = tokenizer.encode(input_text)\n",
        "token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n",
        "start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
        "all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n",
        "# a nice puppet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "par ##vez\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuiIlk2oaSZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"'Hi! . My name is Parvez . \\\n",
        "Skilled Pyhton programmer with experience in Data Analysis and NLP. Firm skills in\\\n",
        "data pre-processing and visualization. Good understanding of popular data mining and machine\\\n",
        "learning techniques. Quick learner and love challenging problems.'\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQNzMRgTY1uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question = \"Descrive yourself\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYAGEXLZe3bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request, json \n",
        "with urllib.request.urlopen(\"http://convai.io/data/data_intermediate.json\") as url:\n",
        "    data = json.loads(url.read().decode())\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfMy5AUnifzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaCXHdo8jSES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQt9qjy6llli",
        "colab_type": "text"
      },
      "source": [
        "# Conv AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8CeZgi9lowt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/huggingface/transfer-learning-conv-ai\n",
        "%cd transfer-learning-conv-ai\n",
        "!pip install -r requirements.txt\n",
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3wMDBkfl4vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 interact.py --model openai-gpt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBBVQ94Ew6gZ",
        "colab_type": "code",
        "outputId": "b93d70e0-0e03-4819-cec6-d9ef231d5e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import random, string\n",
        "password = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(10))\n",
        "#Print root password\n",
        "print(\"Root password: {}\".format(password))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root password: xnS1HH0p9v\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvoVG-MbnPir",
        "colab_type": "code",
        "outputId": "f99fd2d1-72d4-4d39-98a4-92337e9a7d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "\n",
        "\n",
        "# #Download ngrok\n",
        "! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip -qq -n ngrok-stable-linux-amd64.zip\n",
        "#Setup sshd\n",
        "! apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n",
        "# #Set root password\n",
        "! echo root:$password | chpasswd\n",
        "! mkdir -p /var/run/sshd\n",
        "! echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n",
        "! echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\n",
        "! echo \"LD_LIBRARY_PATH=/usr/lib64-nvidia\" >> /root/.bashrc\n",
        "! echo \"export LD_LIBRARY_PATH\" >> /root/.bashrc\n",
        "\n",
        "#Run sshd\n",
        "get_ipython().system_raw('/usr/sbin/sshd -D &')\n",
        "\n",
        "#Ask token\n",
        "print(\"Copy authtoken from https://dashboard.ngrok.com/auth\")\n",
        "import getpass\n",
        "authtoken = getpass.getpass()\n",
        "\n",
        "# #Create tunnel\n",
        "get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Creating config file /etc/ssh/sshd_config with new version\n",
            "Creating SSH2 RSA key; this may take some time ...\n",
            "2048 SHA256:tFw87kpOa8z6wkrfdHRyv6ApDFmFAitqau279/8VR/8 root@a815aff7c06f (RSA)\n",
            "Creating SSH2 ECDSA key; this may take some time ...\n",
            "256 SHA256:nRvNQJ1vDz6YRH8++HVAB0ZPi4+pZ/5Ed7t7w3ss8Ks root@a815aff7c06f (ECDSA)\n",
            "Creating SSH2 ED25519 key; this may take some time ...\n",
            "256 SHA256:tiVyaGN7qqN/NoqM7+MB/rtwMeykwU20XpPkgkMkW+g root@a815aff7c06f (ED25519)\n",
            "Created symlink /etc/systemd/system/sshd.service → /lib/systemd/system/ssh.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/ssh.service → /lib/systemd/system/ssh.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Copy authtoken from https://dashboard.ngrok.com/auth\n",
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_HTOopeu32q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd /content/\n",
        "# %cp /content/transfer-learning-conv-ai/ngrok /content\n",
        "# !./ngrok tcp 22\n",
        "# get_ipython().system_raw('./ngrok tcp 22 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANNEbmMBvHWx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98e4e850-bd22-45ce-9ec3-7a5892bae3cc"
      },
      "source": [
        "#Get public address\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcp://0.tcp.ngrok.io:12472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZHfP68wqs8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !apt install pagekite\n",
        "# get_ipython().system_raw('pagekite 22 ssh.pzchatbot.pagekite.me &')\n",
        "# !pagekite 22 ssh.pzchatbot.pagekite.me\n",
        "# !pagekite "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn_jc6ANimz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('/usr/sbin/sshd -D &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ45ovFKVaL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo lsof -i -P -n | grep LISTEN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqMakY8JVcNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ps -A |less"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5om0QMVWR4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b20dbd8-d1d2-4d94-ef37-d5ed52014c81"
      },
      "source": [
        "import requests\n",
        "print(requests.get('http://ip.42.pl/raw').text)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35.236.144.253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDUNkrqCgsDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7rLb1vknvVR",
        "colab_type": "text"
      },
      "source": [
        "# aiohttp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-a44Dftn0y1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7226727-173e-470a-ac6d-34269d8384ce"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2lL06ugn3or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install aiohttp[speedups]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOrcCDULx1L4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp main.py /content/transfer-learning-conv-ai"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iujaFsMXoG9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b4e9d3e-16e0-4f42-aebf-72c7d6cefd95"
      },
      "source": [
        "%%writefile /content/transfer-learning-conv-ai/main.py\n",
        "\n",
        "from aiohttp import web\n",
        "from interact import run\n",
        "routes = web.RouteTableDef()\n",
        "\n",
        "@routes.get('/')\n",
        "async def hello(request):\n",
        "    return web.Response(text=\"Hello, There .... input after / ie: https://pzchatbot.pagekite.me/your input\")\n",
        "\n",
        "@routes.get('/{input}')\n",
        "async def variable_handler(request):\n",
        "    return web.Response(\n",
        "        text=\"replay>, {}\".format(run(request.match_info['input'])))\n",
        "app = web.Application()\n",
        "app.add_routes(routes)\n",
        "web.run_app(app,port=7070)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/transfer-learning-conv-ai/main.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hibwd9ZBo7Ke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e447771-9ed9-4db0-dede-3f57c7c16dc7"
      },
      "source": [
        "%%writefile /content/transfer-learning-conv-ai/interact.py\n",
        "\n",
        "# # Copyright (c) 2019-present, HuggingFace Inc.\n",
        "# All rights reserved.\n",
        "# This source code is licensed under the BSD-style license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "import logging\n",
        "import random\n",
        "from argparse import ArgumentParser\n",
        "from itertools import chain\n",
        "from pprint import pformat\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pytorch_transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from train import SPECIAL_TOKENS, build_input_from_segments, add_special_tokens_\n",
        "from utils import get_dataset, download_pretrained_model\n",
        "\n",
        "def top_filtering(logits, top_k=0., top_p=0.9, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
        "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
        "                whose total probability mass is greater than or equal to the threshold top_p.\n",
        "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
        "                the threshold top_p.\n",
        "            threshold: a minimal threshold to keep logits\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
        "    top_k = min(top_k, logits.size(-1))\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        # Compute cumulative probabilities of sorted tokens\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Back to unsorted indices and set them to -infinity\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    indices_to_remove = logits < threshold\n",
        "    logits[indices_to_remove] = filter_value\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(personality, history, tokenizer, model, args, current_output=None):\n",
        "    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
        "    if current_output is None:\n",
        "        current_output = []\n",
        "\n",
        "    for i in range(args.max_length):\n",
        "        instance = build_input_from_segments(personality, history, current_output, tokenizer, with_eos=False)\n",
        "\n",
        "        input_ids = torch.tensor(instance[\"input_ids\"], device=args.device).unsqueeze(0)\n",
        "        token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=args.device).unsqueeze(0)\n",
        "\n",
        "        logits = model(input_ids, token_type_ids=token_type_ids)\n",
        "        if isinstance(logits, tuple):  # for gpt2 and maybe others\n",
        "            logits = logits[0]\n",
        "        logits = logits[0, -1, :] / args.temperature\n",
        "        logits = top_filtering(logits, top_k=args.top_k, top_p=args.top_p)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        prev = torch.topk(probs, 1)[1] if args.no_sample else torch.multinomial(probs, 1)\n",
        "        if i < args.min_length and prev.item() in special_tokens_ids:\n",
        "            while prev.item() in special_tokens_ids:\n",
        "                if probs.max().item() == 1:\n",
        "                    warnings.warn(\"Warning: model generating special token with probability 1.\")\n",
        "                    break  # avoid infinitely looping over special token\n",
        "                prev = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        if prev.item() in special_tokens_ids:\n",
        "            break\n",
        "        current_output.append(prev.item())\n",
        "\n",
        "    return current_output\n",
        "\n",
        "def run(input):\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument(\"--dataset_path\", type=str, default=\"\", help=\"Path or url of the dataset. If empty download from S3.\")\n",
        "    parser.add_argument(\"--dataset_cache\", type=str, default='./dataset_cache', help=\"Path or url of the dataset cache\")\n",
        "    parser.add_argument(\"--model\", type=str, default=\"openai-gpt\", help=\"Model type (openai-gpt or gpt2)\", choices=['openai-gpt', 'gpt2'])  # anything besides gpt2 will load openai-gpt\n",
        "    parser.add_argument(\"--model_checkpoint\", type=str, default=\"\", help=\"Path, url or short name of the model\")\n",
        "    parser.add_argument(\"--max_history\", type=int, default=2, help=\"Number of previous utterances to keep in history\")\n",
        "    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"Device (cuda or cpu)\")\n",
        "\n",
        "    parser.add_argument(\"--no_sample\", action='store_true', help=\"Set to use greedy decoding instead of sampling\")\n",
        "    parser.add_argument(\"--max_length\", type=int, default=20, help=\"Maximum length of the output utterances\")\n",
        "    parser.add_argument(\"--min_length\", type=int, default=1, help=\"Minimum length of the output utterances\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=0, help=\"Seed\")\n",
        "    parser.add_argument(\"--temperature\", type=int, default=0.7, help=\"Sampling softmax temperature\")\n",
        "    parser.add_argument(\"--top_k\", type=int, default=0, help=\"Filter top-k tokens before sampling (<=0: no filtering)\")\n",
        "    parser.add_argument(\"--top_p\", type=float, default=0.9, help=\"Nucleus filtering (top-p) before sampling (<=0.0: no filtering)\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logger = logging.getLogger(__file__)\n",
        "    logger.info(pformat(args))\n",
        "\n",
        "    if args.model_checkpoint == \"\":\n",
        "        if args.model == 'gpt2':\n",
        "            raise ValueError(\"Interacting with GPT2 requires passing a finetuned model_checkpoint\")\n",
        "        else:\n",
        "            args.model_checkpoint = download_pretrained_model()\n",
        "\t\n",
        "\t\n",
        "    if args.seed != 0:\n",
        "    \trandom.seed(args.seed)\n",
        "    \ttorch.random.manual_seed(args.seed)\n",
        "    \ttorch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "    logger.info(\"Get pretrained model and tokenizer\")\n",
        "    tokenizer_class, model_class = (GPT2Tokenizer, GPT2LMHeadModel) if args.model == 'gpt2' else (OpenAIGPTTokenizer, OpenAIGPTLMHeadModel)\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.model_checkpoint)\n",
        "    model = model_class.from_pretrained(args.model_checkpoint)\n",
        "    model.to(args.device)\n",
        "    add_special_tokens_(model, tokenizer)\n",
        "\n",
        "    logger.info(\"Sample a personality\")\n",
        "    dataset = get_dataset(tokenizer, args.dataset_path, args.dataset_cache)\n",
        "    personalities = [dialog[\"personality\"] for dataset in dataset.values() for dialog in dataset]\n",
        "    personality = random.choice(personalities)\n",
        "    logger.info(\"Selected personality: %s\", tokenizer.decode(chain(*personality)))\n",
        "\n",
        "    history = []\n",
        "    # while True:\n",
        "    #     raw_text = input(\">>> \")\n",
        "    #     while not raw_text:\n",
        "    #         print('Prompt should not be empty!')\n",
        "    #         raw_text = input(\">>> \")\n",
        "    #     history.append(tokenizer.encode(raw_text))\n",
        "    #     with torch.no_grad():\n",
        "    #         out_ids = sample_sequence(personality, history, tokenizer, model, args)\n",
        "    #     history.append(out_ids)\n",
        "    #     history = history[-(2*args.max_history+1):]\n",
        "    #     out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
        "    #     print(out_text)\n",
        "\n",
        "    \n",
        "    raw_text = input\n",
        "    logger.info(\"user input:{}\".format(raw_text))\n",
        "    history.append(tokenizer.encode(raw_text))\n",
        "    with torch.no_grad():\n",
        "        out_ids = sample_sequence(personality, history, tokenizer, model, args)\n",
        "    history.append(out_ids)\n",
        "    history = history[-(2*args.max_history+1):]\n",
        "    out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
        "    return out_text\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     run()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/transfer-learning-conv-ai/interact.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwNyG8X-7d8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "47b952cf-5cab-42b0-a19f-e3c2963c81bd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wKJQtSqphsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %pwd\n",
        "%cp -r transfer-learning-conv-ai/ \"/content/drive/My Drive/random\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AifyeL071rE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}